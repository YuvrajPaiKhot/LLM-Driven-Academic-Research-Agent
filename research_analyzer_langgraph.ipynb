{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7da029e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from docx import Document\n",
    "\n",
    "# Define the state structure\n",
    "class ResearchState(TypedDict):\n",
    "    raw_query: str\n",
    "    query: str\n",
    "    scraped_articles: dict\n",
    "    summarized_articles: dict\n",
    "    document_path: str\n",
    "    messages: Annotated[list, add_messages]\n",
    "    current_step: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c02ef187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_node(state: ResearchState) -> dict:\n",
    "    \"\"\"Node for scraping articles from Google Scholar\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f\"https://scholar.google.com/scholar?q={query}%20ieee\")\n",
    "\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, \".gs_rt a\")\n",
    "    old_title = driver.title\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    all_docs = {}\n",
    "\n",
    "    for i in range(len(elements)):\n",
    "        ele = elements[i]\n",
    "        ele.click()\n",
    "        wait.until(lambda d: d.title != old_title)\n",
    "        new_title = driver.title\n",
    "\n",
    "        access = driver.find_elements(By.CLASS_NAME, \"document-access-icon\")\n",
    "\n",
    "        if len(access) > 0:\n",
    "            article = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"ArticlePage\")))\n",
    "            paras = article.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "            doc_text = \"\"\n",
    "            for p in paras:\n",
    "                doc_text = doc_text + \" \" + p.text \n",
    "            \n",
    "            all_docs[new_title] = doc_text\n",
    "        else:\n",
    "            all_docs[new_title] = \"Document is locked\"\n",
    "            \n",
    "        driver.back()\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".gs_rt\")))\n",
    "        elements = driver.find_elements(By.CSS_SELECTOR, \".gs_rt a\")\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return {\n",
    "        \"scraped_articles\": all_docs,\n",
    "        \"current_step\": \"scraping_complete\",\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": f\"Scraped {len(all_docs)} articles\"}]\n",
    "    }\n",
    "\n",
    "def summarize_articles_node(state: ResearchState) -> dict:\n",
    "    \"\"\"Node for summarizing scraped articles\"\"\"\n",
    "    doc_dict = state[\"scraped_articles\"]\n",
    "    summarized_dict = {}\n",
    "\n",
    "    for doc_title, doc_content in doc_dict.items():\n",
    "        if doc_content != \"Document is locked\":\n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json={\n",
    "                    \"model\": \"mistral\",\n",
    "                    \"prompt\": f\"Summarize the following research article in 2-3 paragraphs, focusing on key findings and methodology:\\n\\n{doc_content}\\n\\nSummary:\",\n",
    "                    \"stream\": False\n",
    "                }\n",
    "            )\n",
    "            summarized_dict[doc_title] = response.json()[\"response\"]\n",
    "        else:\n",
    "            summarized_dict[doc_title] = \"Article was not accessible for summarization\"\n",
    "\n",
    "    return {\n",
    "        \"summarized_articles\": summarized_dict,\n",
    "        \"current_step\": \"summarization_complete\",\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": f\"Summarized {len(summarized_dict)} articles\"}]\n",
    "    }\n",
    "\n",
    "def create_document_node(state: ResearchState) -> dict:\n",
    "    \"\"\"Node for creating the final Word document\"\"\"\n",
    "    summarized_dict = state[\"summarized_articles\"]\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    doc = Document()\n",
    "    doc.add_heading(f'Research Analysis: {query}', 0)\n",
    "    doc.add_paragraph(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    doc.add_paragraph(\"\")\n",
    "\n",
    "    for title, summary in summarized_dict.items():\n",
    "        doc.add_heading(title, level=1)\n",
    "        doc.add_paragraph(summary)\n",
    "        doc.add_paragraph(\"\")\n",
    "\n",
    "    file_path = state[\"document_path\"]\n",
    "    doc.save(file_path)\n",
    "    \n",
    "    return {\n",
    "        \"document_path\": file_path,\n",
    "        \"current_step\": \"document_created\",\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": f\"Document saved as {file_path}\"}]\n",
    "    }\n",
    "\n",
    "def prompt_parser_node(state: ResearchState) -> str:\n",
    "    \"\"\"Node for extracting search query from user prompt\"\"\"\n",
    "    prompt = state[\"raw_query\"]\n",
    "\n",
    "    research_query = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"mistral\",\n",
    "                \"prompt\": f\"\"\"Extract a concise research topic from this user prompt: \"{prompt}\"\n",
    "\n",
    "                    Rules:\n",
    "                    - Return only the main research topic (2-5 words)\n",
    "                    - Remove filler words like \"find\", \"search\", \"papers about\"\n",
    "                    - Focus on the core subject matter\n",
    "                    - Make it suitable for academic search\n",
    "\n",
    "                    Examples:\n",
    "                    - \"Find recent papers on AI agents in healthcare\" → \"AI agents healthcare\"\n",
    "                    - \"I want research about machine learning for climate change\" → \"machine learning climate change\"\n",
    "\n",
    "                    User prompt: {prompt}\n",
    "                    Research topic:\"\"\",\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    research_query_text = research_query.json()[\"response\"]\n",
    "    return {\n",
    "        \"query\": research_query_text,\n",
    "        \"current_step\": \"query_extraction_complete\",\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": f\"extracted query: {research_query_text} from prompt: {prompt}\"}]\n",
    "    }\n",
    "\n",
    "def should_continue_to_summarize(state: ResearchState) -> str:\n",
    "    \"\"\"Conditional edge function\"\"\"\n",
    "    if state[\"current_step\"] == \"scraping_complete\" and state[\"scraped_articles\"]:\n",
    "        return \"summarize\"\n",
    "    return \"end\"\n",
    "\n",
    "def should_continue_to_document(state: ResearchState) -> str:\n",
    "    \"\"\"Conditional edge function\"\"\"\n",
    "    if state[\"current_step\"] == \"summarization_complete\" and state[\"summarized_articles\"]:\n",
    "        return \"create_document\"\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a60661e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_research_workflow():\n",
    "    \"\"\"Create and compile the research analysis workflow\"\"\"\n",
    "    \n",
    "    # Initialize the graph\n",
    "    workflow = StateGraph(ResearchState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"parse_prompt\", prompt_parser_node)\n",
    "    workflow.add_node(\"scrape_articles\", scrape_articles_node)\n",
    "    workflow.add_node(\"summarize_articles\", summarize_articles_node)\n",
    "    workflow.add_node(\"create_document\", create_document_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"parse_prompt\")\n",
    "\n",
    "    workflow.add_edge(\"parse_prompt\", \"scrape_articles\")\n",
    "    \n",
    "    # Add conditional edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"scrape_articles\",\n",
    "        should_continue_to_summarize,\n",
    "        {\n",
    "            \"summarize\": \"summarize_articles\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"summarize_articles\", \n",
    "        should_continue_to_document,\n",
    "        {\n",
    "            \"create_document\": \"create_document\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"create_document\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Create the workflow\n",
    "research_workflow = create_research_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfcb37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_research_analysis(prompt: str):\n",
    "    \"\"\"Run the complete research analysis workflow\"\"\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"raw_query\": prompt,\n",
    "        \"query\": \"\",\n",
    "        \"scraped_articles\": {},\n",
    "        \"summarized_articles\": {},\n",
    "        \"document_path\": r\"C:\\Users\\yuvra\\OneDrive\\Desktop\\ML Labs\\Projects\\Browser Automation\\Selenium Tutorial\\summarized_article.docx\",\n",
    "        \"messages\": [],\n",
    "        \"current_step\": \"starting\"\n",
    "    }\n",
    "    \n",
    "    # Execute the workflow\n",
    "    final_state = research_workflow.invoke(initial_state)\n",
    "    \n",
    "    print(f\"Analysis complete!\")\n",
    "    print(f\"Original prompt: {final_state['raw_query']}\")\n",
    "    print(f\"Extracted query: {final_state['query']}\")\n",
    "    print(f\"Scraped articles: {len(final_state['scraped_articles'])}\")\n",
    "    print(f\"Summarized articles: {len(final_state['summarized_articles'])}\")\n",
    "    print(f\"Document saved at: {final_state['document_path']}\")\n",
    "    \n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9e16f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete!\n",
      "Original prompt: Research machine learning applications in early disease detection using medical imaging\n",
      "Extracted query:  Machine Learning Applications for Early Disease Detection via Medical Imaging\n",
      "Scraped articles: 10\n",
      "Summarized articles: 10\n",
      "Document saved at: C:\\Users\\yuvra\\OneDrive\\Desktop\\ML Labs\\Projects\\Browser Automation\\Selenium Tutorial\\summarized_article.docx\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "result = run_research_analysis(\"Research machine learning applications in early disease detection using medical imaging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace642b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
